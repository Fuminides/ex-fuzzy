{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "\n",
    "# In case yo urun this without installing the package, you need to add the path to the package\n",
    "sys.path.append('./ex_fuzzy/')\n",
    "sys.path.append('../ex_fuzzy/')\n",
    "\n",
    "import ex_fuzzy.fuzzy_sets as fs\n",
    "import ex_fuzzy.evolutionary_fit as GA\n",
    "import ex_fuzzy.utils as  utils\n",
    "import ex_fuzzy.eval_tools as eval_tools\n",
    "import ex_fuzzy.persistence as persistence\n",
    "import ex_fuzzy.vis_rules as vis_rules\n",
    "import ex_fuzzy.classifiers as classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 1 # 1: single thread, 2+: corresponding multi-thread\n",
    "\n",
    "n_gen = 100\n",
    "n_pop = 50\n",
    "    \n",
    "nRules = 15\n",
    "nAnts = 4\n",
    "vl = 3\n",
    "tolerance = 0.1\n",
    "fz_type_studied = fs.FUZZY_SETS.t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the fuzzy partitions using 3 quartiles\n",
    "precomputed_partitions = utils.construct_partitions(X, fz_type_studied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "n_gen  |  n_eval  |     f_avg     |     f_min    \n",
      "=================================================\n",
      "     1 |       50 |  0.6488976823 |  0.4173051266\n",
      "     2 |      100 |  0.5192343045 |  0.4173051266\n",
      "     3 |      150 |  0.4617103662 |  0.3668859826\n",
      "     4 |      200 |  0.4264902541 |  0.3020444737\n",
      "     5 |      250 |  0.4005345631 |  0.3020444737\n",
      "     6 |      300 |  0.3752225938 |  0.3020444737\n",
      "     7 |      350 |  0.3412898545 |  0.3020444737\n",
      "     8 |      400 |  0.3033539602 |  0.3020444737\n",
      "     9 |      450 |  0.3020444737 |  0.3020444737\n",
      "    10 |      500 |  0.3020444737 |  0.3020444737\n",
      "    11 |      550 |  0.3020444737 |  0.3020444737\n",
      "    12 |      600 |  0.3020444737 |  0.3020444737\n",
      "    13 |      650 |  0.3020444737 |  0.3020444737\n",
      "    14 |      700 |  0.3020444737 |  0.3020444737\n",
      "    15 |      750 |  0.3020444737 |  0.3020444737\n",
      "    16 |      800 |  0.3020444737 |  0.3020444737\n",
      "    17 |      850 |  0.3020444737 |  0.3020444737\n",
      "    18 |      900 |  0.3020444737 |  0.3020444737\n",
      "    19 |      950 |  0.3020444737 |  0.3020444737\n",
      "    20 |     1000 |  0.3020444737 |  0.3020444737\n",
      "    21 |     1050 |  0.3020444737 |  0.3020444737\n",
      "    22 |     1100 |  0.3020444737 |  0.3020444737\n",
      "    23 |     1150 |  0.3020444737 |  0.3020444737\n",
      "    24 |     1200 |  0.3020444737 |  0.3020444737\n",
      "    25 |     1250 |  0.3020444737 |  0.3020444737\n",
      "    26 |     1300 |  0.3020444737 |  0.3020444737\n",
      "    27 |     1350 |  0.3020444737 |  0.3020444737\n",
      "    28 |     1400 |  0.3020444737 |  0.3020444737\n",
      "    29 |     1450 |  0.3020444737 |  0.3020444737\n",
      "    30 |     1500 |  0.3020444737 |  0.3020444737\n",
      "    31 |     1550 |  0.3020444737 |  0.3020444737\n",
      "    32 |     1600 |  0.3020444737 |  0.3020444737\n",
      "    33 |     1650 |  0.3020444737 |  0.3020444737\n",
      "    34 |     1700 |  0.3020444737 |  0.3020444737\n",
      "    35 |     1750 |  0.3020444737 |  0.3020444737\n",
      "    36 |     1800 |  0.3020444737 |  0.3020444737\n",
      "    37 |     1850 |  0.3020444737 |  0.3020444737\n",
      "    38 |     1900 |  0.3020444737 |  0.3020444737\n",
      "    39 |     1950 |  0.3020444737 |  0.3020444737\n",
      "    40 |     2000 |  0.3020444737 |  0.3020444737\n",
      "    41 |     2050 |  0.3020444737 |  0.3020444737\n",
      "    42 |     2100 |  0.3020444737 |  0.3020444737\n",
      "    43 |     2150 |  0.3020444737 |  0.3020444737\n",
      "    44 |     2200 |  0.3020444737 |  0.3020444737\n",
      "    45 |     2250 |  0.3020444737 |  0.3020444737\n",
      "    46 |     2300 |  0.3020444737 |  0.3020444737\n",
      "    47 |     2350 |  0.3020444737 |  0.3020444737\n",
      "    48 |     2400 |  0.3020444737 |  0.3020444737\n",
      "    49 |     2450 |  0.3020444737 |  0.3020444737\n",
      "    50 |     2500 |  0.3020444737 |  0.3020444737\n",
      "    51 |     2550 |  0.3020444737 |  0.3020444737\n",
      "    52 |     2600 |  0.3020444737 |  0.3020444737\n",
      "    53 |     2650 |  0.3020444737 |  0.3020444737\n",
      "    54 |     2700 |  0.3020444737 |  0.3020444737\n",
      "    55 |     2750 |  0.3020444737 |  0.3020444737\n",
      "    56 |     2800 |  0.3020444737 |  0.3020444737\n",
      "    57 |     2850 |  0.3020444737 |  0.3020444737\n",
      "    58 |     2900 |  0.3020444737 |  0.3020444737\n",
      "    59 |     2950 |  0.3020444737 |  0.3020444737\n",
      "    60 |     3000 |  0.3020444737 |  0.3020444737\n",
      "    61 |     3050 |  0.3020444737 |  0.3020444737\n",
      "    62 |     3100 |  0.3020444737 |  0.3020444737\n",
      "    63 |     3150 |  0.3020444737 |  0.3020444737\n",
      "    64 |     3200 |  0.3020444737 |  0.3020444737\n",
      "    65 |     3250 |  0.3020444737 |  0.3020444737\n",
      "    66 |     3300 |  0.3020444737 |  0.3020444737\n",
      "    67 |     3350 |  0.3020444737 |  0.3020444737\n",
      "    68 |     3400 |  0.3020444737 |  0.3020444737\n",
      "    69 |     3450 |  0.3020444737 |  0.3020444737\n",
      "    70 |     3500 |  0.3020444737 |  0.3020444737\n",
      "    71 |     3550 |  0.3020444737 |  0.3020444737\n",
      "    72 |     3600 |  0.3020444737 |  0.3020444737\n",
      "    73 |     3650 |  0.3020444737 |  0.3020444737\n",
      "    74 |     3700 |  0.3020444737 |  0.3020444737\n",
      "    75 |     3750 |  0.3020444737 |  0.3020444737\n",
      "    76 |     3800 |  0.3020444737 |  0.3020444737\n",
      "    77 |     3850 |  0.3020444737 |  0.3020444737\n",
      "    78 |     3900 |  0.3020444737 |  0.3020444737\n",
      "    79 |     3950 |  0.3020444737 |  0.3020444737\n",
      "    80 |     4000 |  0.3020444737 |  0.3020444737\n",
      "    81 |     4050 |  0.3020444737 |  0.3020444737\n",
      "    82 |     4100 |  0.3020444737 |  0.3020444737\n",
      "    83 |     4150 |  0.3020444737 |  0.3020444737\n",
      "    84 |     4200 |  0.3020444737 |  0.3020444737\n",
      "    85 |     4250 |  0.3020444737 |  0.3020444737\n",
      "    86 |     4300 |  0.3020444737 |  0.3020444737\n",
      "    87 |     4350 |  0.3020444737 |  0.3020444737\n",
      "    88 |     4400 |  0.3020444737 |  0.3020444737\n",
      "    89 |     4450 |  0.3020444737 |  0.3020444737\n",
      "    90 |     4500 |  0.3020444737 |  0.3020444737\n",
      "    91 |     4550 |  0.3020444737 |  0.3020444737\n",
      "    92 |     4600 |  0.3020444737 |  0.3020444737\n",
      "    93 |     4650 |  0.3020444737 |  0.3020444737\n",
      "    94 |     4700 |  0.3020444737 |  0.3020444737\n",
      "    95 |     4750 |  0.3020444737 |  0.3020444737\n",
      "    96 |     4800 |  0.3020444737 |  0.3020444737\n",
      "    97 |     4850 |  0.3020444737 |  0.3020444737\n",
      "    98 |     4900 |  0.3020444737 |  0.3020444737\n",
      "    99 |     4950 |  0.3020444737 |  0.3020444737\n",
      "   100 |     5000 |  0.3020444737 |  0.3020444737\n",
      "------------\n",
      "ACCURACY\n",
      "Train performance: 0.72\n",
      "Test performance: 0.66\n",
      "------------\n",
      "MATTHEW CORRCOEF\n",
      "Train performance: 0.6118494443793504\n",
      "Test performance: 0.5660366158474326\n",
      "------------\n",
      "Rules for consequent: 0\n",
      "----------------\n",
      "IF sepal length (cm) IS Low AND sepal width (cm) IS High WITH DS 0.5621131031169467, ACC 1.0\n",
      "IF sepal width (cm) IS Medium AND petal length (cm) IS Low WITH DS 0.13827711688214306, ACC 1.0\n",
      "\n",
      "Rules for consequent: 1\n",
      "----------------\n",
      "IF sepal length (cm) IS Medium AND petal length (cm) IS Low WITH DS 0.3171049558984879, ACC 0.9375\n",
      "\n",
      "Rules for consequent: 2\n",
      "----------------\n",
      "IF sepal length (cm) IS Medium WITH DS 0.12225988039233142, ACC 0.6666666666666666\n",
      "IF sepal width (cm) IS Medium WITH DS 0.2529721458770988, ACC 0.6666666666666666\n",
      "IF sepal width (cm) IS High WITH DS 0.10112373265676385, ACC 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the RuleMine classifier\n",
    "fl_classifier = classifiers.RuleMineClassifier(nRules=nRules, nAnts=nAnts, fuzzy_type=fz_type_studied, linguistic_variables=precomputed_partitions,\n",
    "                                               verbose=False, tolerance=tolerance, runner=threads)\n",
    "fl_classifier.fit(X_train, y_train, n_gen=n_gen, pop_size=n_pop)\n",
    "\n",
    "str_rules = eval_tools.eval_fuzzy_model(fl_classifier.internal_classifier(), X_train, y_train, X_test, y_test, \n",
    "                        plot_rules=False, print_rules=True, plot_partitions=False, return_rules=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1 |       50 |  0.8825637749 |  0.4816610370\n",
      "     2 |      100 |  0.7013196761 |  0.4816610370\n",
      "     3 |      150 |  0.6197672254 |  0.4816610370\n",
      "     4 |      200 |  0.5587990855 |  0.4608277036\n",
      "     5 |      250 |  0.4927363117 |  0.4608277036\n",
      "     6 |      300 |  0.4783277036 |  0.4608277036\n",
      "     7 |      350 |  0.4720777036 |  0.4608277036\n",
      "     8 |      400 |  0.4608277036 |  0.4608277036\n",
      "     9 |      450 |  0.4608277036 |  0.4608277036\n",
      "    10 |      500 |  0.4608068703 |  0.4597860370\n",
      "    11 |      550 |  0.4607652036 |  0.4597860370\n",
      "    12 |      600 |  0.4606181178 |  0.4586817421\n",
      "    13 |      650 |  0.4602822793 |  0.4586817421\n",
      "    14 |      700 |  0.4595872639 |  0.4586817421\n",
      "    15 |      750 |  0.4591976421 |  0.4572234088\n",
      "    16 |      800 |  0.4585067421 |  0.4572234088\n",
      "    17 |      850 |  0.4582442421 |  0.4572234088\n",
      "    18 |      900 |  0.4575734088 |  0.4572234088\n",
      "    19 |      950 |  0.4572234088 |  0.4572234088\n",
      "    20 |     1000 |  0.4572234088 |  0.4572234088\n",
      "    21 |     1050 |  0.4572234088 |  0.4572234088\n",
      "    22 |     1100 |  0.4572234088 |  0.4572234088\n",
      "    23 |     1150 |  0.4572234088 |  0.4572234088\n",
      "    24 |     1200 |  0.4572234088 |  0.4572234088\n",
      "    25 |     1250 |  0.4572234088 |  0.4572234088\n",
      "    26 |     1300 |  0.4572234088 |  0.4572234088\n",
      "    27 |     1350 |  0.4571817421 |  0.4551400755\n",
      "    28 |     1400 |  0.4570984088 |  0.4551400755\n",
      "    29 |     1450 |  0.4568900755 |  0.4551400755\n",
      "    30 |     1500 |  0.4564734088 |  0.4551400755\n",
      "    31 |     1550 |  0.4554317421 |  0.4551400755\n",
      "    32 |     1600 |  0.4551400755 |  0.4551400755\n",
      "    33 |     1650 |  0.4551400755 |  0.4551400755\n",
      "    34 |     1700 |  0.4551400755 |  0.4551400755\n",
      "    35 |     1750 |  0.4551400755 |  0.4551400755\n",
      "    36 |     1800 |  0.4551400755 |  0.4551400755\n",
      "    37 |     1850 |  0.4551400755 |  0.4551400755\n",
      "    38 |     1900 |  0.4551400755 |  0.4551400755\n",
      "    39 |     1950 |  0.4551400755 |  0.4551400755\n",
      "    40 |     2000 |  0.4551400755 |  0.4551400755\n",
      "    41 |     2050 |  0.4551400755 |  0.4551400755\n",
      "    42 |     2100 |  0.4551400755 |  0.4551400755\n",
      "    43 |     2150 |  0.4551400755 |  0.4551400755\n",
      "    44 |     2200 |  0.4551400755 |  0.4551400755\n",
      "    45 |     2250 |  0.4551400755 |  0.4551400755\n",
      "    46 |     2300 |  0.4551400755 |  0.4551400755\n",
      "    47 |     2350 |  0.4551400755 |  0.4551400755\n",
      "    48 |     2400 |  0.4551400755 |  0.4551400755\n",
      "    49 |     2450 |  0.4551400755 |  0.4551400755\n",
      "    50 |     2500 |  0.4551400755 |  0.4551400755\n",
      "    51 |     2550 |  0.4551400755 |  0.4551400755\n",
      "    52 |     2600 |  0.4551400755 |  0.4551400755\n",
      "    53 |     2650 |  0.4551400755 |  0.4551400755\n",
      "    54 |     2700 |  0.4551400755 |  0.4551400755\n",
      "    55 |     2750 |  0.4551400755 |  0.4551400755\n",
      "    56 |     2800 |  0.4551400755 |  0.4551400755\n",
      "    57 |     2850 |  0.4551400755 |  0.4551400755\n",
      "    58 |     2900 |  0.4551400755 |  0.4551400755\n",
      "    59 |     2950 |  0.4551400755 |  0.4551400755\n",
      "    60 |     3000 |  0.4551400755 |  0.4551400755\n",
      "    61 |     3050 |  0.4551400755 |  0.4551400755\n",
      "    62 |     3100 |  0.4551400755 |  0.4551400755\n",
      "    63 |     3150 |  0.4551400755 |  0.4551400755\n",
      "    64 |     3200 |  0.4551400755 |  0.4551400755\n",
      "    65 |     3250 |  0.4551400755 |  0.4551400755\n",
      "    66 |     3300 |  0.4551400755 |  0.4551400755\n",
      "    67 |     3350 |  0.4551400755 |  0.4551400755\n",
      "    68 |     3400 |  0.4551400755 |  0.4551400755\n",
      "    69 |     3450 |  0.4551400755 |  0.4551400755\n",
      "    70 |     3500 |  0.4551400755 |  0.4551400755\n",
      "    71 |     3550 |  0.4551400755 |  0.4551400755\n",
      "    72 |     3600 |  0.4551400755 |  0.4551400755\n",
      "    73 |     3650 |  0.4551400755 |  0.4551400755\n",
      "    74 |     3700 |  0.4551400755 |  0.4551400755\n",
      "    75 |     3750 |  0.4551400755 |  0.4551400755\n",
      "    76 |     3800 |  0.4551400755 |  0.4551400755\n",
      "    77 |     3850 |  0.4551400755 |  0.4551400755\n",
      "    78 |     3900 |  0.4551400755 |  0.4551400755\n",
      "    79 |     3950 |  0.4551400755 |  0.4551400755\n",
      "    80 |     4000 |  0.4551400755 |  0.4551400755\n",
      "    81 |     4050 |  0.4551400755 |  0.4551400755\n",
      "    82 |     4100 |  0.4551400755 |  0.4551400755\n",
      "    83 |     4150 |  0.4551400755 |  0.4551400755\n",
      "    84 |     4200 |  0.4551400755 |  0.4551400755\n",
      "    85 |     4250 |  0.4551400755 |  0.4551400755\n",
      "    86 |     4300 |  0.4551400755 |  0.4551400755\n",
      "    87 |     4350 |  0.4551400755 |  0.4551400755\n",
      "    88 |     4400 |  0.4551400755 |  0.4551400755\n",
      "    89 |     4450 |  0.4551400755 |  0.4551400755\n",
      "    90 |     4500 |  0.4551400755 |  0.4551400755\n",
      "    91 |     4550 |  0.4551400755 |  0.4551400755\n",
      "    92 |     4600 |  0.4551400755 |  0.4551400755\n",
      "    93 |     4650 |  0.4551400755 |  0.4551400755\n",
      "    94 |     4700 |  0.4551400755 |  0.4551400755\n",
      "    95 |     4750 |  0.4551400755 |  0.4551400755\n",
      "    96 |     4800 |  0.4551400755 |  0.4551400755\n",
      "    97 |     4850 |  0.4551400755 |  0.4551400755\n",
      "    98 |     4900 |  0.4551400755 |  0.4551400755\n",
      "    99 |     4950 |  0.4551400755 |  0.4551400755\n",
      "   100 |     5000 |  0.4551400755 |  0.4551400755\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaseFuzzyRulesClassifier.fit() got an unexpected keyword argument 'n_runner'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the FuzzyRules classifier\u001b[39;00m\n\u001b[0;32m      2\u001b[0m fl_classifier \u001b[38;5;241m=\u001b[39m classifiers\u001b[38;5;241m.\u001b[39mFuzzyRulesClassifier(nRules\u001b[38;5;241m=\u001b[39mnRules, nAnts\u001b[38;5;241m=\u001b[39mnAnts, fuzzy_type\u001b[38;5;241m=\u001b[39mfz_type_studied, linguistic_variables\u001b[38;5;241m=\u001b[39mprecomputed_partitions,\n\u001b[0;32m      3\u001b[0m                                                verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tolerance\u001b[38;5;241m=\u001b[39mtolerance, runner\u001b[38;5;241m=\u001b[39mthreads)\n\u001b[1;32m----> 4\u001b[0m fl_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, n_gen\u001b[38;5;241m=\u001b[39mn_gen, pop_size\u001b[38;5;241m=\u001b[39mn_pop)\n\u001b[0;32m      6\u001b[0m str_rules \u001b[38;5;241m=\u001b[39m eval_tools\u001b[38;5;241m.\u001b[39meval_fuzzy_model(fl_classifier\u001b[38;5;241m.\u001b[39minternal_classifier(), X_train, y_train, X_test, y_test, \n\u001b[0;32m      7\u001b[0m                         plot_rules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, print_rules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, plot_partitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_rules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\javi-\\anaconda3\\envs\\datasci\\Lib\\site-packages\\ex_fuzzy\\classifiers.py:126\u001b[0m, in \u001b[0;36mFuzzyRulesClassifier.fit\u001b[1;34m(self, X, y, n_gen, pop_size, checkpoints, n_runner)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfl_classifier1\u001b[38;5;241m.\u001b[39mfit(X, y, n_gen, pop_size, checkpoints)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase1_rules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfl_classifier1\u001b[38;5;241m.\u001b[39mrule_base\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfl_classifier2\u001b[38;5;241m.\u001b[39mfit(X, y, n_gen, pop_size, checkpoints, initial_rules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase1_rules, n_runner\u001b[38;5;241m=\u001b[39mn_runner)\n",
      "\u001b[1;31mTypeError\u001b[0m: BaseFuzzyRulesClassifier.fit() got an unexpected keyword argument 'n_runner'"
     ]
    }
   ],
   "source": [
    "# Create the FuzzyRules classifier\n",
    "fl_classifier = classifiers.FuzzyRulesClassifier(nRules=nRules, nAnts=nAnts, fuzzy_type=fz_type_studied, linguistic_variables=precomputed_partitions,\n",
    "                                               verbose=False, tolerance=tolerance, runner=threads)\n",
    "fl_classifier.fit(X_train, y_train, n_gen=n_gen, pop_size=n_pop)\n",
    "\n",
    "str_rules = eval_tools.eval_fuzzy_model(fl_classifier.internal_classifier(), X_train, y_train, X_test, y_test, \n",
    "                        plot_rules=False, print_rules=True, plot_partitions=False, return_rules=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RuleFineTuneClassifier classifier\n",
    "fl_classifier = classifiers.RuleFineTuneClassifier(nRules=nRules, nAnts=nAnts, fuzzy_type=fz_type_studied, linguistic_variables=precomputed_partitions,\n",
    "                                               verbose=False, tolerance=tolerance, runner=threads)\n",
    "fl_classifier.fit(X_train, y_train, n_gen=n_gen, pop_size=n_pop)\n",
    "\n",
    "str_rules = eval_tools.eval_fuzzy_model(fl_classifier.internal_classifier(), X_train, y_train, X_test, y_test, \n",
    "                        plot_rules=False, print_rules=True, plot_partitions=False, return_rules=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
